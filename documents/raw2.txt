Documentación de lo realizado en la conversación:
Inicio del scraping:

Se comenzó con el objetivo de realizar un scraping de propiedades desde el sitio web de RE/MAX Ecuador. Se trabajó con un script de Python utilizando las librerías requests y BeautifulSoup para obtener datos sobre propiedades, como el precio, expensas, dirección, superficies, ambientes, baños y más.
Selección de elementos HTML:

A lo largo de la conversación se analizaron ejemplos de HTML correspondientes a las propiedades de la página. Esto incluyó diferentes divs y selectores donde se ubicaban los datos necesarios, como el precio, dirección y características de la propiedad. Se probaron varios selectores CSS para extraer los datos correctos.
Ajuste del nombre de archivos de salida:

Se ajustó el script para que los archivos de salida se guarden en la carpeta data/output con un nombre de formato remax_pag_fecha_hora.txt, donde se incluía la fecha y hora actual en formato ddmmaa_hhmm.
Guardado del HTML de las propiedades:

Se implementó un sistema para guardar el HTML de las tarjetas de propiedades obtenidas de cada página. Cada archivo contenía información detallada sobre las propiedades obtenidas de la página, con una separación clara para cada una.
Formato de CSV para los datos obtenidos:

Se diseñó un CSV que incluye campos como Precio, Expensas, Dirección, Superficie Total, Superficie Cubierta, Ambientes, Baños, Descripción, Agente, y Oficina. Se ajustó el CSV para que cada propiedad ocupe una sola línea, separando los valores con | para evitar confusiones con los caracteres.
Corrección y mejora del CSV:

Se hicieron mejoras en el script para que los encabezados se incluyan correctamente y se garantice que todas las propiedades se registren en una sola línea. Además, se evaluó si se podían extraer más campos útiles, como información adicional o detalles de contacto.
Iteraciones sobre el scraping y más datos:

Durante la conversación, se revisaron ejemplos adicionales de propiedades para ver si era posible extraer más datos, como información sobre las características de las propiedades y otros detalles relevantes que puedan mejorar el dataset.
Preparación de scripts reutilizables:

Se finalizó con un script que permite realizar el scraping en varias páginas, especificando la ciudad y la cantidad de páginas a iterar. El script extrae los datos relevantes y los guarda en un archivo de texto y en un CSV para un procesamiento más sencillo en el futuro.
De esta manera, se completó una solución para el scraping de propiedades en la página de RE/MAX, adaptada a las necesidades de extracción de datos y almacenamiento.



DOCS DE EXTRACTORES

1. Modificación y creación de extractores de datos para portales inmobiliarios:
Se han modificado y creado extractores de datos para tres portales principales: Properati, Fazwaz y Plusvalía. El objetivo principal fue desarrollar scripts en Python que permitieran la extracción de información de propiedades desde archivos HTML descargados. Los extractores se encargan de:

Identificar archivos HTML en directorios específicos.
Extraer atributos relevantes de cada propiedad (precio, ubicación, habitaciones, baños, área, precio por metro cuadrado, entre otros).
Generar archivos CSV con la información extraída, manteniendo la estructura de los archivos originales y agregando la fecha y hora del procesamiento en el nombre de los archivos CSV.
1.1. Extractores para Properati y Fazwaz:
Properati: Se implementó un extractor que obtiene datos desde HTML descargados. El script busca archivos HTML en la carpeta de entrada y guarda los resultados en formato CSV en la carpeta de salida. Los atributos extraídos incluyen: precio, ubicación, habitaciones, baños, área, precio por m², fecha de publicación y descripción. El nombre del archivo CSV de salida conserva la fecha y hora de procesamiento.

Fazwaz: Similar a Properati, se creó un extractor que recoge atributos clave de las propiedades como precio, ubicación, características del inmueble y otras especificaciones relevantes. El archivo de salida también sigue el formato CSV y conserva la fecha y hora.

1.2. Plusvalía Extract:
Plusvalía: En este caso, el script procesa archivos HTML descargados con información de propiedades de Plusvalía. Se extrajeron atributos como precio, ubicación, habitaciones, baños, área, precio por metro cuadrado, fecha de publicación y descripciones. Se añadió la capacidad de buscar automáticamente todos los archivos HTML que comiencen con "Plusvalia" en la carpeta de origen y se aseguró que los archivos CSV de salida mantengan la fecha y hora en su nombre.
2. Modificaciones y mejoras en los extractores:
Se revisaron y ajustaron las clases CSS y etiquetas HTML utilizadas en el proceso de scraping para que correspondan con la estructura de los archivos HTML de cada plataforma.
Se implementó una lógica para normalizar las listas de atributos (completar listas con valores "N/A" en caso de que falten algunos datos) y asegurar que los archivos CSV tengan una estructura consistente.
Los extractores fueron diseñados para ser reutilizables y modulares, lo que permite fácilmente agregar más archivos o ajustar la extracción según cambios futuros en las plataformas.
3. Corrección de errores en scripts:
Se solucionaron errores relacionados con la localización de archivos, como rutas incorrectas o problemas de búsqueda de archivos HTML.
Se ajustaron las rutas de salida para que los archivos CSV se guarden en las carpetas correspondientes (data/csv/).
Se eliminaron campos innecesarios como el "ID" y la "fecha de captura" en los archivos CSV finales, siguiendo las preferencias del usuario.
4. Documentación adicional:
Para cada script, se añadió una función que imprime los atributos de las propiedades extraídas, permitiendo una fácil verificación visual del contenido procesado.
Este conjunto de scripts ahora permite un scraping eficiente y procesamiento de información de propiedades desde archivos HTML descargados, con salida en formato CSV organizada y estructurada para análisis posterior.
